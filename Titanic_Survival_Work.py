# -*- coding: utf-8 -*-
"""Untitled117.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ue65onWcnhXy8vjRo6TB3rn0uxjDzjDg
"""

# -------------------------
# 1. Imports & settings
# -------------------------
import os
import warnings
warnings.filterwarnings("ignore")

import pandas as pd
import numpy as np
from pathlib import Path
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,
                             roc_auc_score, precision_recall_curve, roc_curve,
                             classification_report, average_precision_score)
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.decomposition import PCA

from scipy.stats import gaussian_kde

# SHAP & PDF (safe import)
try:
    import shap
    shap_available = True
except Exception:
    shap_available = False
    print("shap not available. SHAP plots will be skipped (install shap if needed).")

#PDF (safe import)
try:
    from fpdf import FPDF
    fpdf_available = True
except Exception:
    fpdf_available = False
    print("fpdf not available. PDF report generation will be skipped (install fpdf if needed).")

# Repro
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)
sns.set_theme(style="darkgrid")

# Create models dir
os.makedirs("/content/models", exist_ok=True)

# -------------------------
# 2. Utility functions
# -------------------------
def load_data_colab(path="/content/Titanic-Dataset.csv"):
    p = Path(path)
    if not p.exists():
        raise FileNotFoundError(f"{path} not found. Upload your Titanic-Dataset.csv to /content or update the path.")
    return pd.read_csv(p)

def save_model_colab(model, feature_list, name="/content/models/titanic_model"):
    joblib.dump(model, f"{name}.joblib")
    joblib.dump(feature_list, f"{name}.features.joblib")
    print(f"Saved model: {name}.joblib and features: {name}.features.joblib")

def print_missing(df):
    print("Missing per column:")
    print(df.isnull().sum())

# -------------------------
# 3. Load data
# -------------------------
DATA_PATH = "/content/Titanic-Dataset.csv"  # Colab default upload path
df_raw = load_data_colab(DATA_PATH)
print("Loaded dataset shape:", df_raw.shape)
display(df_raw.head())
print_missing(df_raw)

# Make a copy of the raw DataFrame to work with
df = df_raw.copy()

# Fill missing Age with median
df['Age'].fillna(df['Age'].median(), inplace=True)

# Fill missing Embarked with mode
df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)

# Extract deck from Cabin and fill missing as 'Unknown'
df['Deck'] = df['Cabin'].str[0].fillna('U')
df.drop('Cabin', axis=1, inplace=True)

# -------------------------
# 4. Preprocessing & Feature Engineering (robust)
# -------------------------
def preprocess_colab(df, drop_passengerid=True):
    df = df.copy()
    # Impute Age & Fare
    df['Age'] = df['Age'].fillna(df['Age'].median())
    df['Fare'] = df['Fare'].fillna(df['Fare'].median())
    if 'Embarked' in df.columns:
        df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])
    # Cabin -> Deck
    df['Cabin'] = df.get('Cabin', 'Unknown').fillna('Unknown')
    df['Deck'] = df['Cabin'].astype(str).str[0].replace('n','U').fillna('U')
    # Title
    if 'Name' in df.columns:
        df['Title'] = df['Name'].str.extract(r' ([A-Za-z]+)\.', expand=False)
        df['Title'] = df['Title'].replace({'Mlle':'Miss','Ms':'Miss','Mme':'Mrs'})
        rare_titles = ['Lady','Countess','Capt','Col','Don','Dr','Major','Rev','Sir','Jonkheer','Dona']
        df['Title'] = df['Title'].replace(rare_titles, 'Rare').fillna('Unknown')
    # Family
    df['FamilySize'] = df.get('SibSp',0) + df.get('Parch',0) + 1
    df['IsAlone'] = (df['FamilySize']==1).astype(int)
    # AgeGroup & FareBin
    df['AgeGroup'] = pd.cut(df['Age'], bins=[0,12,20,35,60,120], labels=['Child','Teen','YoungAdult','Adult','Senior'])
    df['FareBin'] = pd.qcut(df['Fare'].rank(method='first'), 4, labels=['Low','Med','High','VeryHigh'])
    # Drop raw columns
    drop_cols = [c for c in ['Ticket','Cabin','Name'] if c in df.columns]
    if drop_passengerid and 'PassengerId' in df.columns:
        drop_cols.append('PassengerId')
    df = df.drop(columns=drop_cols, errors='ignore')

    # Define features
    categorical_features = [c for c in ['Sex','Embarked','Pclass','Title','Deck','AgeGroup','FareBin'] if c in df.columns]
    numerical_features = ['Age','Fare','FamilySize']

    # ColumnTransformer
    numeric_transformer = Pipeline([('scaler', StandardScaler())])
    categorical_transformer = Pipeline([('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])

    preprocessor = ColumnTransformer([
        ('num', numeric_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ], remainder='passthrough')

    # Fit transform (exclude target if present)
    if 'Survived' in df.columns:
        Xtmp = df.drop(columns=['Survived'])
    else:
        Xtmp = df.copy()

    X_trans = preprocessor.fit_transform(Xtmp)

    # feature names
    num_names = numerical_features
    try:
        ohe = preprocessor.named_transformers_['cat'].named_steps['ohe']
        cat_names = list(ohe.get_feature_names_out(categorical_features))
    except Exception:
        cat_names = []
    remainder_cols = [c for c in Xtmp.columns if c not in numerical_features + categorical_features]
    feature_names = num_names + cat_names + remainder_cols

    df_processed = pd.DataFrame(X_trans, columns=feature_names, index=Xtmp.index)
    if 'Survived' in df.columns:
        df_processed['Survived'] = df['Survived'].values
    if 'IsAlone' not in df_processed.columns:
        df_processed['IsAlone'] = df.get('IsAlone', 0)

    return df_processed, preprocessor, feature_names

df_proc, preproc_obj, feat_names = preprocess_colab(df_raw)
print("Processed shape:", df_proc.shape)
display(df_proc.head())

# -------------------------
# 5. Train/test split
# -------------------------
if 'Survived' not in df_proc.columns:
    raise ValueError("Preprocessed data missing 'Survived' target.")
X = df_proc.drop(columns=['Survived'])
y = df_proc['Survived'].astype(int)
FEATURE_LIST = X.columns.tolist()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,
                                                    random_state=RANDOM_STATE, stratify=y)
print("Train/Test shapes:", X_train.shape, X_test.shape)

RANDOM_STATE = 42  # make sure this is defined

# Define models
models = {
    "LogisticRegression": LogisticRegression(max_iter=400, random_state=RANDOM_STATE),
    "RandomForest": RandomForestClassifier(n_estimators=120, random_state=RANDOM_STATE),
    "GradientBoosting": GradientBoostingClassifier(random_state=RANDOM_STATE),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=RANDOM_STATE)
}

# Train models
trained = {}
for name, m in models.items():
    print(f"Training {name} ...")
    m.fit(X_train, y_train)
    trained[name] = m

# Quick GridSearch (small) for RF & XGB
print("GridSearch (small) for RandomForest...")
rf_grid = {"n_estimators": [100, 120], "max_depth": [None, 6]}
gs_rf = GridSearchCV(RandomForestClassifier(random_state=RANDOM_STATE),
                     rf_grid, cv=2, scoring='accuracy', n_jobs=-1)
gs_rf.fit(X_train, y_train)
trained['RandomForest_Tuned'] = gs_rf.best_estimator_
print("RF best params:", gs_rf.best_params_)

print("GridSearch (small) for XGBoost...")
xgb_grid = {"n_estimators": [100, 150], "max_depth": [3, 4], "learning_rate": [0.05, 0.1]}
gs_xgb = GridSearchCV(
    XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=RANDOM_STATE),
    xgb_grid, cv=2, scoring='accuracy', n_jobs=-1
)
gs_xgb.fit(X_train, y_train)
trained['XGBoost_Tuned'] = gs_xgb.best_estimator_
print("XGB best params:", gs_xgb.best_params_)

# -------------------------
# 7. Evaluation: metrics & comparison
# -------------------------
def evaluate_model_local(model, X_t, y_t):
    y_pred = model.predict(X_t)
    try:
        y_proba = model.predict_proba(X_t)[:,1]
    except Exception:
        y_proba = None
    metrics = {
        "accuracy": accuracy_score(y_t, y_pred),
        "precision": precision_score(y_t, y_pred, zero_division=0),
        "recall": recall_score(y_t, y_pred, zero_division=0),
        "f1": f1_score(y_t, y_pred, zero_division=0),
        "roc_auc": roc_auc_score(y_t, y_proba) if y_proba is not None else np.nan,
        "pr_auc": average_precision_score(y_t, y_proba) if y_proba is not None else np.nan
    }
    return metrics, y_pred, y_proba

eval_results = []
roc_map = {}
pr_map = {}
for name, model in trained.items():
    print("Evaluating:", name)
    m, y_p, y_pv = evaluate_model_local(model, X_test, y_test)
    m['model'] = name
    eval_results.append(m)
    if y_pv is not None:
        fpr, tpr, _ = roc_curve(y_test, y_pv)
        roc_map[name] = (fpr, tpr)
        prec, rec, _ = precision_recall_curve(y_test, y_pv)
        pr_map[name] = (prec, rec)

eval_df = pd.DataFrame(eval_results).sort_values("roc_auc", ascending=False, na_position='last')
display(eval_df)

# Show classification reports for top 4
top = eval_df['model'].iloc[:4].tolist()
for t in top:
    print(f"\nClassification report - {t}")
    print(classification_report(y_test, trained[t].predict(X_test), zero_division=0))

# -------------------------
# 8. ROC & Precision-Recall plots (colab-friendly)
# -------------------------
plt.figure(figsize=(9,6))
for name,(fpr,tpr) in roc_map.items():
    auc = roc_auc_score(y_test, trained[name].predict_proba(X_test)[:,1])
    plt.plot(fpr, tpr, label=f"{name} (AUC={auc:.3f})", lw=2)
plt.plot([0,1],[0,1],'k--', alpha=0.4)
plt.title("ROC Comparison")
plt.xlabel("FPR"); plt.ylabel("TPR"); plt.legend(); plt.grid(True)
plt.show()

plt.figure(figsize=(9,6))
for name,(prec,rec) in pr_map.items():
    ap = average_precision_score(y_test, trained[name].predict_proba(X_test)[:,1])
    plt.plot(rec, prec, label=f"{name} (AP={ap:.3f})", lw=2)
plt.title("Precision-Recall Comparison")
plt.xlabel("Recall"); plt.ylabel("Precision"); plt.legend(); plt.grid(True)
plt.show()

# -------------------------
# 9. Feature importance (RF & XGB) + optional SHAP (optimized sampling)
# -------------------------
def plot_feature_importance_colab(model, feature_names, top_n=20, title="Feature Importance"):
    if not hasattr(model, "feature_importances_"):
        print("Model has no feature_importances_")
        return
    imp = model.feature_importances_
    fi = pd.DataFrame({"feature":feature_names, "importance":imp}).sort_values("importance", ascending=False).head(top_n)
    plt.figure(figsize=(8, max(4,0.3*len(fi))))
    sns.barplot(x='importance', y='feature', data=fi, palette='viridis')
    plt.title(title); plt.tight_layout(); plt.show()

if 'RandomForest_Tuned' in trained:
    plot_feature_importance_colab(trained['RandomForest_Tuned'], FEATURE_LIST, title="RF Tuned Importance")
if 'XGBoost_Tuned' in trained:
    plot_feature_importance_colab(trained['XGBoost_Tuned'], FEATURE_LIST, title="XGB Tuned Importance")

# SHAP (TreeExplainer, sample limited to 200 rows to fit Colab RAM)
if shap_available:
    print("Running SHAP (sample=200)...")
    explainer = shap.TreeExplainer(trained.get('XGBoost_Tuned', trained.get('XGBoost')))
    sample_idx = X_test.sample(n=min(200, X_test.shape[0]), random_state=RANDOM_STATE).index
    X_shap = X_test.loc[sample_idx]
    shap_values = explainer.shap_values(X_shap)
    # summary
    try:
        shap.summary_plot(shap_values, X_shap, show=True)
    except Exception as e:
        print("SHAP summary plot error:", e)
else:
    print("SHAP not installed; skipping SHAP visuals.")

# -------------------------
# 10. Advanced 3D Plotly Visuals
# -------------------------
# 3D scatter (Age, Fare, Pclass), color by Survived, size by Fare
if {'Age','Fare','Pclass','Survived'}.issubset(df_raw.columns):
    fig = px.scatter_3d(
        df_raw.dropna(subset=['Age','Fare','Pclass']),
        x='Age', y='Fare', z='Pclass',
        color='Survived', symbol='Sex' if 'Sex' in df_raw.columns else None,
        size='Fare', opacity=0.75,
        title='3D Survival Landscape: Age vs Fare vs Pclass'
    )
    fig.update_layout(margin=dict(t=40)); fig.show()

# 3D animated frames by Pclass (use string frames for Plotly)
if {'Age','Fare','FamilySize','Pclass','Survived'}.issubset(df_proc.columns):
    df_anim = df_raw.copy()
    if 'FamilySize' not in df_anim.columns:
        df_anim = df_anim.join(df_proc['FamilySize'], how='left')
    df_anim = df_anim.dropna(subset=['Age','Fare','FamilySize','Pclass'])
    df_anim['Pclass_str'] = df_anim['Pclass'].astype(str)
    fig = px.scatter_3d(df_anim, x='Age', y='Fare', z='FamilySize', color='Survived',
                        animation_frame='Pclass_str', title='Animated 3D: Age vs Fare vs FamilySize (frames=Pclass)')
    fig.update_layout(margin=dict(t=40)); fig.show()

# 3D KDE surface (Age vs Fare density) - grid reduced for colab
if {'Age','Fare'}.issubset(df_raw.columns):
    data = df_raw.dropna(subset=['Age','Fare'])
    x = data['Age'].values; y = data['Fare'].values
    try:
        k = gaussian_kde(np.vstack([x,y]))
        xi, yi = np.mgrid[x.min():x.max():60j, y.min():y.max():60j]
        zi = k(np.vstack([xi.flatten(), yi.flatten()])).reshape(xi.shape)
        surf = go.Figure(data=[go.Surface(x=xi, y=yi, z=zi, colorscale='Viridis', opacity=0.9)])
        surf.update_layout(title='3D Density Surface (Age vs Fare)', scene=dict(xaxis_title='Age', yaxis_title='Fare', zaxis_title='Density'))
        surf.show()
    except Exception as e:
        print("3D KDE error:", e)

# PCA 3D scatter (reduced markers)
pca = PCA(n_components=3)
# Sample df_proc directly to ensure X and y are aligned
df_sampled_for_pca = df_proc.sample(n=min(800, df_proc.shape[0]), random_state=RANDOM_STATE)
X_sampled_for_pca = df_sampled_for_pca.drop(columns=['Survived'])
y_sampled_for_pca = df_sampled_for_pca['Survived'].values

pca_feats = pca.fit_transform(X_sampled_for_pca)
df_pca = pd.DataFrame(pca_feats, columns=['PC1','PC2','PC3'])
df_pca['Survived'] = y_sampled_for_pca
fig = px.scatter_3d(df_pca, x='PC1', y='PC2', z='PC3', color='Survived', title='3D PCA (sampled)')
fig.update_traces(marker=dict(size=3)); fig.show()

# -------------------------
# 12. Optional: generate PDF report (Colab) - skipped
# -------------------------
# PDF generation is skipped; we just print summary in notebook
summary_text = ("This report summarizes preprocessing, feature engineering, "
                "model training (LogisticRegression, RandomForest, GradientBoosting, XGBoost), "
                "and evaluation. SHAP explainability was included if shap was installed.")

print("\n=== FINAL SUMMARY ===")
display(eval_df)  # show model evaluation table
print("\nSummary:")
print(summary_text)

if not shap_available:
    print("\nTo enable SHAP explainability: pip install shap")

# No file paths, no saving, no downloads

